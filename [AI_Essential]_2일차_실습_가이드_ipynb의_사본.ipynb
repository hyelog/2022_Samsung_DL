{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyelog/2022_Samsung_DL/blob/main/%5BAI_Essential%5D_2%EC%9D%BC%EC%B0%A8_%EC%8B%A4%EC%8A%B5_%EA%B0%80%EC%9D%B4%EB%93%9C_ipynb%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "987b0d09",
      "metadata": {
        "id": "987b0d09"
      },
      "source": [
        "# 03. 인공지능"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8e6585",
      "metadata": {
        "id": "4a8e6585"
      },
      "source": [
        "## 03-001 모듈  설정\n",
        "- 이 실습에서는 PyTorch와 JAEN 라이브러리에서 필요한 모듈을 임포트하는 과정을 학습합니다\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6259bba8",
      "metadata": {
        "id": "6259bba8"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install JAEN -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6a6f1d9",
      "metadata": {
        "id": "f6a6f1d9"
      },
      "outputs": [],
      "source": [
        "# 모듈 설정\n",
        "import torch\n",
        "from JAEN.utils import plot_activation_function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8221abf9",
      "metadata": {
        "id": "8221abf9"
      },
      "source": [
        "- PyTorch와 JAEN 패키지에서 필요한 모듈을 설정하고 활성화 함수 시각화 함수를 불러오는 과정을 실습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8b47e38",
      "metadata": {
        "id": "d8b47e38"
      },
      "source": [
        "## 03-002 시그모이드 활성화 함수 구현 및 시각화\n",
        "- 이 실습에서는 시그모이드(sigmoid) 활성화 함수를 직접 구현하고, 해당 함수를 시각화하는 방법을 학습합니다. 시그모이드 함수는 입력값을 0과 1 사이의 값으로 변환하며, 뉴런의 활성화를 결정하는 데 사용됩니다. 이후, plot_activation_function() 함수를 사용하여 시그모이드 함수의 그래프를 시각화합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f1b816d",
      "metadata": {
        "id": "4f1b816d"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + torch.exp(-x))\n",
        "\n",
        "plot_activation_function(sigmoid)\n",
        "plot_activation_function(torch.sigmoid)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74ed44bc",
      "metadata": {
        "id": "74ed44bc"
      },
      "source": [
        "- 시그모이드 활성화 함수를 구현하고, 이를 시각화하는 과정을 실습합니다. 시그모이드 함수는 신경망에서 자주 사용되는 비선형 활성화 함수입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9970c9cf",
      "metadata": {
        "id": "9970c9cf"
      },
      "source": [
        "## 03-003 Tanh 활성화 함수 구현 및 시각화\n",
        "- 이 실습에서는 하이퍼볼릭 탄젠트(Tanh) 활성화 함수를 직접 구현하고, 이를 시각화하는 방법을 학습합니다. Tanh 함수는 입력값을 -1과 1 사이로 변환하며, 시그모이드 함수보다 기울기가 더 큰 비선형 활성화 함수입니다. 이후, plot_activation_function() 함수를 사용하여 Tanh 함수의 그래프를 시각화합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9014c772",
      "metadata": {
        "id": "9014c772"
      },
      "outputs": [],
      "source": [
        "def tanh(x):\n",
        "    return (torch.exp(x) - torch.exp(-x)) / (torch.exp(x) + torch.exp(-x))\n",
        "\n",
        "plot_activation_function(tanh)\n",
        "plot_activation_function(torch.tanh)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "347cc452",
      "metadata": {
        "id": "347cc452"
      },
      "source": [
        "- Tanh 활성화 함수를 구현하고, 이를 시각화하는 과정을 실습합니다. Tanh 함수는 신경망에서 자주 사용되는 비선형 활성화 함수로, 시그모이드 함수와 유사한 역할을 수행합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6753494c",
      "metadata": {
        "id": "6753494c"
      },
      "source": [
        "## 03-004 ReLU 활성화 함수 구현 및 시각화\n",
        "- 이 실습에서는 ReLU(Rectified Linear Unit) 활성화 함수를 직접 구현하고, 이를 시각화하는 방법을 학습합니다. ReLU 함수는 입력값이 0보다 작으면 0을 출력하고, 0보다 크면 그 값을 그대로 출력하는 함수입니다. ReLU는 현재 딥러닝에서 가장 널리 사용되는 활성화 함수 중 하나입니다. 이후, plot_activation_function() 함수를 사용하여 ReLU 함수의 그래프를 시각화합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dacbc400",
      "metadata": {
        "id": "dacbc400"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return torch.max(torch.tensor(0.0), x)\n",
        "\n",
        "plot_activation_function(relu)\n",
        "plot_activation_function(torch.relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b0780fd",
      "metadata": {
        "id": "6b0780fd"
      },
      "source": [
        "- ReLU 활성화 함수를 구현하고, 이를 시각화하는 과정을 실습합니다. ReLU는 비선형성을 제공하며, 신경망 학습에서 중요한 역할을 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5425e67b",
      "metadata": {
        "id": "5425e67b"
      },
      "source": [
        "## 03-005 Leaky ReLU 활성화 함수 구현 및 시각화\n",
        "- 이 실습에서는 Leaky ReLU 활성화 함수를 직접 구현하고, 이를 시각화하는 방법을 학습합니다. Leaky ReLU는 ReLU의 변형으로, 입력값이 0보다 작을 때도 작은 기울기(alpha 값)를 허용하여 죽은 뉴런 문제를 완화합니다. torch.where() 함수를 사용하여 양수일 때는 x를, 음수일 때는 alpha * x를 반환합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "482b1753",
      "metadata": {
        "id": "482b1753"
      },
      "outputs": [],
      "source": [
        "def leaky_relu(x, alpha=0.01):\n",
        "    return torch.max(x, alpha * x)\n",
        "\n",
        "plot_activation_function(leaky_relu)\n",
        "plot_activation_function(torch.nn.functional.leaky_relu)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76b9d237",
      "metadata": {
        "id": "76b9d237"
      },
      "source": [
        "- Leaky ReLU 활성화 함수를 구현하고, 이를 시각화하는 과정을 실습합니다. Leaky ReLU는 ReLU 함수의 단점을 보완한 활성화 함수로, 기울기 소실 문제를 줄이는 데 유용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ee067a0",
      "metadata": {
        "id": "5ee067a0"
      },
      "source": [
        "## 03-006 ELU 활성화 함수 구현 및 시각화\n",
        "- 이 실습에서는 ELU(Exponential Linear Unit) 활성화 함수를 구현하고, 이를 시각화하는 방법을 학습합니다. ELU 함수는 ReLU와 유사하지만, 음수 값에 대해서는 exponential 함수를 사용해 부드러운 곡선을 제공합니다. alpha는 음수 구간에서의 기울기를 조정하는 매개변수입니다. torch.where()를 사용해 양수일 때는 x를, 음수일 때는 alpha * (torch.exp(x) 1)을 반환합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01825b4f",
      "metadata": {
        "id": "01825b4f"
      },
      "outputs": [],
      "source": [
        "def elu(x, alpha=1.0):\n",
        "    return torch.where(x > 0, x, alpha * (torch.exp(x) - 1))\n",
        "\n",
        "plot_activation_function(elu)\n",
        "plot_activation_function(torch.nn.functional.elu)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "519c0086",
      "metadata": {
        "id": "519c0086"
      },
      "source": [
        "- ELU 활성화 함수를 구현하고, 이를 시각화하는 과정을 실습합니다. ELU는 음수 구간에서 더 부드러운 비선형성을 제공하여 신경망 학습에서 유용하게 사용됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ef15e6f",
      "metadata": {
        "id": "3ef15e6f"
      },
      "source": [
        "## 03-007 requires_grad=True로 설정된 텐서 생성\n",
        "- 이 실습에서는 텐서 생성 시 requires_grad=True로 설정하여 해당 텐서의 연산에 대한 기울기를 자동으로 계산하도록 지정하는 방법을 학습합니다. 이 설정을 통해 텐서가 연산 그래프에서 기울기(gradient)를 추적하게 되어 역전파(backpropagation)를 수행할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46eb7bb7",
      "metadata": {
        "id": "46eb7bb7"
      },
      "outputs": [],
      "source": [
        "# requires_grad=True로 설정된 텐서 생성\n",
        "x = torch.tensor([2.0, 3.0], requires_grad=True)\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "083ab9f2",
      "metadata": {
        "id": "083ab9f2"
      },
      "source": [
        "- requires_grad=True 옵션을 사용하여 기울기 계산이 가능한 텐서를 생성하는 과정을 실습합니다. 이 옵션은 신경망 학습 과정에서 매우 중요한 역할을 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "891ed3f0",
      "metadata": {
        "id": "891ed3f0"
      },
      "source": [
        "## 03-008 텐서의 연산 및 역전파 수행\n",
        "- 이 실습에서는 텐서의 연산과 역전파를 수행하는 방법을 학습합니다. 텐서 x에 대해 제곱 연산을 수행하여 y를 구하고, y의 요소를 합산하여 z를 계산합니다. 이후, z.backward()를 호출하여 역전파를 수행하고, 텐서 x에 대한 기울기를 계산합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa90be49",
      "metadata": {
        "id": "fa90be49"
      },
      "outputs": [],
      "source": [
        "# 텐서의 연산\n",
        "y = x ** 2  # y = [4, 9]\n",
        "z = y.sum()  # z = 13\n",
        "\n",
        "# 역전파 수행\n",
        "z.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "829c782a",
      "metadata": {
        "id": "829c782a"
      },
      "source": [
        "- 텐서의 연산을 수행한 후, 역전파를 통해 기울기 계산을 수행하는 과정을 실습합니다. 역전파는 신경망 학습에서 기울기 업데이트에 중요한 역할을 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef18d23c",
      "metadata": {
        "id": "ef18d23c"
      },
      "source": [
        "## 03-009 텐서의 기울기(gradient) 계산\n",
        "- 이 실습에서는 텐서 x에 대한 기울기(gradient)를 계산하는 방법을 학습합니다. z에 대해 역전파를 수행한 후, x.grad를 사용하여 z가 x에 대해 어떻게 변화하는지(편미분)를 계산합니다. 여기서 x1과 x2 각각에 대해 편미분한 결과는 tensor([4., 6.])입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16ba9765",
      "metadata": {
        "id": "16ba9765"
      },
      "outputs": [],
      "source": [
        "# x에 대한 z의 그래디언트 (z가 x에 대해 어떻게 변화하는지를 계산, 편미분 수행)\n",
        "x.grad  # 출력: tensor([4., 6.])\n",
        "\n",
        "# z = x1^2 + x2^2\n",
        "# dz/dx1 = 2 * x1 = 2 * 2 = 4\n",
        "# dz/dx2 = 2 * x2 = 2 * 3 = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8da192ae",
      "metadata": {
        "id": "8da192ae"
      },
      "source": [
        "- 텐서의 기울기를 계산하고, 각 변수에 대한 편미분 결과를 확인하는 과정을 실습합니다. 기울기 계산은 신경망 학습에서 손실 함수의 변화를 분석하는 데 중요한 역할을 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4aa99171",
      "metadata": {
        "id": "4aa99171"
      },
      "source": [
        "## 03-010 DataLoader와 Dataset 모듈 임포트\n",
        "- 이 실습에서는 PyTorch의 DataLoader와 Dataset 모듈을 임포트하는 방법을 학습합니다. Dataset은 사용자 정의 데이터셋을 만들기 위한 기본 클래스이고, DataLoader는 이 데이터셋을 배치 단위로 로드하는 데 사용됩니다. 이 두 모듈은 대규모 데이터 처리를 효율적으로 수행할 수 있도록 도와줍니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4cc8250",
      "metadata": {
        "id": "a4cc8250"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fbe4404",
      "metadata": {
        "id": "7fbe4404"
      },
      "source": [
        "- PyTorch의 데이터 처리를 위한 기본 모듈인 DataLoader와 Dataset을 임포트하는 과정을 실습합니다. 이를 통해 효율적인 데이터 로딩과 배치 처리를 수행할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "622494eb",
      "metadata": {
        "id": "622494eb"
      },
      "source": [
        "## 03-011 커스텀 데이터셋 클래스 구현\n",
        "- 이 실습에서는 PyTorch의 Dataset 클래스를 상속하여 커스텀 데이터셋을 만드는 방법을 학습합니다. CustomDataset 클래스는 데이터와 레이블을 받아 저장하고, __len__() 메서드는 데이터셋의 크기를 반환하며, __getitem__() 메서드는 인덱스에 해당하는 데이터를 반환합니다. 이를 통해 사용자는 자신만의 데이터셋을 정의하고 DataLoader로 쉽게 처리할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a1f53ae",
      "metadata": {
        "id": "1a1f53ae"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.data[idx]\n",
        "        y = self.labels[idx]\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9ac2379",
      "metadata": {
        "id": "d9ac2379"
      },
      "source": [
        "- Dataset 클래스를 상속하여 커스텀 데이터셋을 정의하는 과정을 실습합니다. 이 과정은 맞춤형 데이터셋을 PyTorch 모델에 전달할 때 유용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7094b24d",
      "metadata": {
        "id": "7094b24d"
      },
      "source": [
        "## 03-012 커스텀 데이터셋과 DataLoader 생성\n",
        "- 이 실습에서는 예시 데이터를 사용하여 커스텀 데이터셋과 DataLoader를 생성하는 방법을 학습합니다. 100개의 샘플로 구성된 3차원 벡터 데이터를 생성하고, 이에 대응하는 이진 레이블을 만듭니다. 그런 다음, CustomDataset 클래스를 사용하여 데이터셋을 만들고, DataLoader를 통해 데이터를 배치 단위로 로드할 수 있도록 설정합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f969810",
      "metadata": {
        "id": "7f969810"
      },
      "outputs": [],
      "source": [
        "# 예시 데이터\n",
        "data = torch.randn(100, 3)  # 100개의 샘플, 각 샘플은 3차원 벡터\n",
        "labels = torch.randint(0, 2, (100,))  # 이진 분류를 위한 100개의 레이블\n",
        "\n",
        "# 데이터셋 및 DataLoader 생성\n",
        "dataset = CustomDataset(data, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bde4b66e",
      "metadata": {
        "id": "bde4b66e"
      },
      "source": [
        "- 예시 데이터를 사용해 커스텀 데이터셋과 DataLoader를 생성하고, 데이터를 배치 단위로 로드하는 과정을 실습합니다. 이 과정은 대규모 데이터셋을 효율적으로 처리할 때 필수적입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "084d10e8",
      "metadata": {
        "id": "084d10e8"
      },
      "source": [
        "## 03-013 DataLoader에서 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 로드된 데이터를 배치 단위로 조회하는 방법을 학습합니다. DataLoader를 리스트로 변환하여 각 배치를 확인할 수 있으며, 첫 번째 배치를 조회하여 해당 배치의 데이터와 레이블을 확인합니다. 이를 통해 데이터셋에서 배치 단위로 데이터를 쉽게 가져올 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fcd3e59",
      "metadata": {
        "id": "3fcd3e59"
      },
      "outputs": [],
      "source": [
        "# DataLoader에서 모든 배치를 리스트로 변환\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[0]  # 첫번째 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd8633be",
      "metadata": {
        "id": "cd8633be"
      },
      "source": [
        "- DataLoader에서 로드된 데이터를 배치 단위로 조회하고, 첫 번째 배치의 데이터를 확인하는 과정을 실습합니다. 배치 처리는 신경망 학습에서 중요한 역할을 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89523d60",
      "metadata": {
        "id": "89523d60"
      },
      "source": [
        "## 03-014 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 마지막 배치를 조회하는 방법을 학습합니다. 리스트의 음수 인덱스를 사용하여 마지막 배치를 가져올 수 있으며, 이를 통해 해당 배치의 데이터와 레이블을 확인할 수 있습니다. 마지막 배치는 전체 데이터셋의 크기와 배치 크기에 따라 일부 요소만 포함될 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb61373",
      "metadata": {
        "id": "cdb61373"
      },
      "outputs": [],
      "source": [
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1be9e3ca",
      "metadata": {
        "id": "1be9e3ca"
      },
      "source": [
        "- DataLoader에서 마지막 배치를 조회하고, 해당 배치의 데이터를 확인하는 과정을 실습합니다. 이는 배치 처리에서 특정한 위치의 데이터를 조회할 때 유용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d180929",
      "metadata": {
        "id": "3d180929"
      },
      "source": [
        "## 03-015 DataLoader에서 변경된 배치 크기로 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader의 배치 크기를 변경한 후, 마지막 배치를 조회하는 방법을 학습합니다. 배치 크기를 7로 설정하여 DataLoader에서 데이터를 로드한 뒤, 마지막 배치를 확인합니다. 변경된 배치 크기는 전체 데이터셋을 나누는 방식에 영향을 미치며, 마지막 배치의 크기는 나머지 데이터에 따라 달라질 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416b35c7",
      "metadata": {
        "id": "416b35c7"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=7)\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d3aa470",
      "metadata": {
        "id": "9d3aa470"
      },
      "source": [
        "- DataLoader의 배치 크기를 변경한 후, 마지막 배치를 조회하여 해당 데이터를 확인하는 과정을 실습합니다. 이는 배치 크기에 따른 데이터 처리의 변화를 이해하는 데 유용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cfd57e9",
      "metadata": {
        "id": "3cfd57e9"
      },
      "source": [
        "## 03-016 셔플된 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 데이터를 셔플한 후 마지막 배치를 조회하는 방법을 학습합니다. DataLoader의 shuffle=True 옵션을 사용하여 데이터를 무작위로 섞은 뒤, 배치 크기를 7로 설정하여 데이터를 로드합니다. 이후, 마지막 배치를 조회하여 셔플된 데이터가 제대로 처리되는지 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "868be074",
      "metadata": {
        "id": "868be074"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=7, shuffle=True)\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddd43eb9",
      "metadata": {
        "id": "ddd43eb9"
      },
      "source": [
        "- 셔플된 DataLoader에서 마지막 배치를 조회하여 데이터를 확인하는 과정을 실습합니다. 셔플은 모델 학습 시 데이터의 순서에 의한 편향을 줄이는 데 유용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d54052b",
      "metadata": {
        "id": "8d54052b"
      },
      "source": [
        "## 03-017 마지막 배치를 버리는 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 배치 크기에 맞지 않는 마지막 배치를 버린 후 남은 마지막 배치를 조회하는 방법을 학습합니다. drop_last=True 옵션을 사용하여 전체 데이터셋에서 남는 데이터가 있을 경우 이를 버리고, 나머지 데이터를 배치로 처리하지 않도록 설정합니다. 이후, 마지막 배치를 조회하여 데이터를 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b6b6d0",
      "metadata": {
        "id": "b6b6b6d0"
      },
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=7, shuffle=True, drop_last=True)\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b483caf",
      "metadata": {
        "id": "5b483caf"
      },
      "source": [
        "- DataLoader에서 drop_last=True 옵션을 사용하여 마지막 남은 데이터를 버린 후, 남은 마지막 배치를 조회하는 과정을 실습합니다. 이는 특정 배치 크기에 맞추어 데이터를 균일하게 처리할 때 유용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "399e7e42",
      "metadata": {
        "id": "399e7e42"
      },
      "source": [
        "## 연습문제-03-001 커스텀 데이터셋과 DataLoader 생성\n",
        "- 이 실습에서는 150개의 샘플과 3개의 클래스 레이블을 가진 새로운 랜덤 데이터를 사용하여 커스텀 데이터셋을 생성하고, DataLoader를 설정하는 방법을 학습합니다. 데이터셋은 4차원 벡터로 구성되며, 이 데이터를 DataLoader를 통해 배치 단위로 처리할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0df88add",
      "metadata": {
        "id": "0df88add"
      },
      "outputs": [],
      "source": [
        "# 새로운 랜덤 데이터 생성\n",
        "data = torch.randn(150, 4)  # 150개의 샘플, 각 샘플은 4차원 벡터\n",
        "labels = torch.randint(0, 3, (150,))  # 3개의 클래스로 분류되는 150개의 레이블\n",
        "\n",
        "# 커스텀 데이터셋과 DataLoader 생성\n",
        "# 커스텀 데이터셋 클래스는 03-019 재활용\n",
        "dataset = CustomDataset(data, labels)\n",
        "\n",
        "# 미니 배치 크기는 5\n",
        "dataloader = DataLoader(dataset, batch_size=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6813dfb1",
      "metadata": {
        "id": "6813dfb1"
      },
      "source": [
        "- 새로운 랜덤 데이터를 사용해 커스텀 데이터셋과 DataLoader를 생성하고, 데이터를 배치 단위로 로드하는 방법을 실습합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b449f607",
      "metadata": {
        "id": "b449f607"
      },
      "source": [
        "## 연습문제-03-002 DataLoader에서 배치 조회\n",
        "- 이 실습에서는 새로운 랜덤 데이터로 생성한 DataLoader에서 첫 번째 배치를 조회하는 방법을 학습합니다. 리스트로 변환된 DataLoader에서 첫 번째 배치의 데이터를 가져와 출력합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a26b4a17",
      "metadata": {
        "id": "a26b4a17"
      },
      "outputs": [],
      "source": [
        "# DataLoader에서 모든 배치를 리스트로 변환\n",
        "all_batches = list(dataloader)\n",
        "\n",
        "# 첫 번째 배치 조회\n",
        "batch_data, batch_labels = all_batches[0]  # 첫번째 배치의 데이터와 레이블 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dc22241",
      "metadata": {
        "id": "6dc22241"
      },
      "source": [
        "- DataLoader에서 새로운 랜덤 데이터를 사용하여 첫 번째 배치의 데이터를 확인하는 방법을 실습합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f98129a1",
      "metadata": {
        "id": "f98129a1"
      },
      "source": [
        "## 연습문제-03-003 DataLoader에서 마지막 배치 조회 (새로운 배치 크기)\n",
        "- 이 실습에서는 DataLoader의 배치 크기를 7로 설정하여 마지막 배치를 조회하는 방법을 학습합니다. 배치 크기가 변경됨에 따라 마지막 배치의 데이터가 어떻게 구성되는지 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c90f7ed",
      "metadata": {
        "id": "2c90f7ed"
      },
      "outputs": [],
      "source": [
        "# DataLoader에서 배치 크기를 7로 설정\n",
        "dataloader = DataLoader(dataset, batch_size=7)\n",
        "\n",
        "# DataLoader에서 마지막 배치 조회\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치의 데이터와 레이블 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fb93108",
      "metadata": {
        "id": "2fb93108"
      },
      "source": [
        "- 배치 크기를 7로 설정한 DataLoader에서 마지막 배치를 조회하는 방법을 실습합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e990127",
      "metadata": {
        "id": "8e990127"
      },
      "source": [
        "## 연습문제-03-004 셔플된 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 DataLoader에서 데이터를 셔플한 후 마지막 배치를 조회하는 방법을 학습합니다. shuffle=True 옵션을 사용하여 데이터를 무작위로 섞어 로드하고, 마지막 배치를 조회하여 셔플된 데이터를 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99cec803",
      "metadata": {
        "id": "99cec803"
      },
      "outputs": [],
      "source": [
        "# DataLoader에서 shuffle=True로 설정하여 셔플된 데이터 로드\n",
        "dataloader = DataLoader(dataset, batch_size=7, shuffle=True)\n",
        "\n",
        "# 셔플된 DataLoader에서 마지막 배치 조회\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치의 데이터와 레이블 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba942962",
      "metadata": {
        "id": "ba942962"
      },
      "source": [
        "- 셔플된 DataLoader에서 마지막 배치를 조회하여 데이터를 확인하는 과정을 실습합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58ca7b64",
      "metadata": {
        "id": "58ca7b64"
      },
      "source": [
        "## 연습문제-03-005 drop_last가 설정된 DataLoader에서 마지막 배치 조회\n",
        "- 이 실습에서는 drop_last=True로 설정된 DataLoader에서 마지막 배치를 조회하는 방법을 학습합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b6d7d74",
      "metadata": {
        "id": "9b6d7d74"
      },
      "outputs": [],
      "source": [
        "# DataLoader에서 batch_size=7, shuffle=True, drop_last=True로 설정하여 셔플된 데이터 로드\n",
        "dataloader = DataLoader(dataset, batch_size=7, shuffle=True, drop_last=True)\n",
        "\n",
        "# 셔플된 DataLoader에서 마지막 배치 조회\n",
        "all_batches = list(dataloader)\n",
        "batch_data, batch_labels = all_batches[-1]  # 마지막 배치의 데이터와 레이블 조회\n",
        "batch_data, batch_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "248d4bd7",
      "metadata": {
        "id": "248d4bd7"
      },
      "source": [
        "- 데이터 개수가 배치 사이즈보다 적은 경우 해당 배치를 삭제하는 DataLoader에서 마지막 배치를 조회하고, 데이터를 확인하는 방법을 실습합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6468de1",
      "metadata": {
        "id": "f6468de1"
      },
      "source": [
        "# 04. 심층신경망(DNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa2faf71",
      "metadata": {
        "id": "fa2faf71"
      },
      "source": [
        "## 04-001 PyTorch 및 관련 모듈 임포트와 device 설정\n",
        "- 이 실습에서는 PyTorch와 관련된 모듈을 임포트하고, 신경망을 GPU 또는 CPU에서 실행할 수 있도록 device를 설정하는 방법을 학습합니다. torch.device()를 사용하여 GPU 사용 가능 여부에 따라 적절한 장치를 선택합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e9fb4b",
      "metadata": {
        "scrolled": true,
        "id": "f2e9fb4b"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install JAEN -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d28015",
      "metadata": {
        "id": "01d28015"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from JAEN.utils import plot_training_results\n",
        "\n",
        "# device 설정 (GPU가 사용 가능하면 GPU로, 그렇지 않으면 CPU 사용)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# 데이터 로더 정의\n",
        "from JAEN.datasets import load_titanic\n",
        "train_loader, test_loader = load_titanic()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d635c9ad",
      "metadata": {
        "id": "d635c9ad"
      },
      "source": [
        "- PyTorch의 다양한 모듈을 임포트하고, 학습에 사용할 장치(GPU 또는 CPU)를 설정하는 과정을 실습합니다. 이를 통해 GPU 가속을 활용한 빠른 연산이 가능해집니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7982f7e2",
      "metadata": {
        "id": "7982f7e2"
      },
      "source": [
        "## 04-002 nn.Sequential 기반 신경망 모델 구성\n",
        "- 이 실습에서는 nn.Sequential()을 사용하여 간단한 신경망 모델을 구성하는 방법을 학습합니다. 이 모델은 두 개의 선형 레이어와 ReLU 그리고 Sigmoid 활성화 함수를 사용합니다. 마지막으로 생존 여부를 출력하는 레이어를 추가합니다. summary() 함수를 사용하여 모델의 구조를 요약하고, 각 레이어의 파라미터 수를 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "044e211e",
      "metadata": {
        "id": "044e211e"
      },
      "outputs": [],
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(7, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 1),\n",
        "    nn.Sigmoid()\n",
        ")\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model.to(device)\n",
        "summary(model, (32, 7))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c69e441",
      "metadata": {
        "id": "3c69e441"
      },
      "source": [
        "- 신경망 모델을 구성하고, 요약 정보를 통해 모델의 구조와 파라미터 수를 확인하는 과정을 실습합니다. 이 과정은 모델의 설계를 이해하고 조정하는 데 중요한 단계입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6888ad9b",
      "metadata": {
        "id": "6888ad9b"
      },
      "source": [
        "## 04-003 nn.Module 기반 신경망 모델 구성\n",
        "- 이 실습에서는 nn.Module을 상속하여 신경망 모델 클래스를 구현합니다. __init__() 메서드에서 두 개의 선형 레이어를 정의하며, forward() 메서드에서 데이터 흐름을 정의합니다. 마지막 출력 레이어는 Sigmoid 활성화 함수를 사용합니다. summary() 함수를 사용하여 모델의 구조를 요약합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95d47ec1",
      "metadata": {
        "id": "95d47ec1"
      },
      "outputs": [],
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(7, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# 모델 인스턴스 생성\n",
        "model = DNN().to(device)\n",
        "summary(model, (32, 7))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "499f9070",
      "metadata": {
        "id": "499f9070"
      },
      "source": [
        "- 타이타닉 생존자 분류를 위한 신경망 모델 클래스를 구현하고, 요약 정보를 통해 모델의 구조와 파라미터 수를 확인하는 과정을 실습합니다. 이 과정은 모델 설계를 이해하고 조정하는 데 중요합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5da6c64b",
      "metadata": {
        "id": "5da6c64b"
      },
      "source": [
        "## 04-004 모델 학습 함수 구현\n",
        "- 이 실습에서는 PyTorch 모델을 학습하기 위한 train() 함수를 구현합니다. 모델을 학습 모드로 설정하고, 데이터 로더에서 배치 단위로 이미지를 가져와 모델의 출력과 손실을 계산합니다. 손실에 대해 역전파를 수행하고 옵티마이저를 사용하여 파라미터를 업데이트합니다. 마지막으로, 전체 손실을 계산하여 반환합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bec3579",
      "metadata": {
        "id": "4bec3579"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    model.train()  # 모델을 학습 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "    for datas, labels in train_loader: # 미니 배치 별 파라미터 업데이트 수행\n",
        "        datas, labels = datas.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "        # 순전파\n",
        "        outputs = model(datas)\n",
        "\n",
        "        # 손실 계산\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 기울기 초기화\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 역전파\n",
        "        loss.backward()\n",
        "\n",
        "        # 파라미터 업데이트\n",
        "        optimizer.step()\n",
        "\n",
        "        # 손실 누적\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    return running_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a67ddb09",
      "metadata": {
        "id": "a67ddb09"
      },
      "source": [
        "- 모델 학습을 위한 train() 함수를 구현하고, 각 에포크에서 손실을 반환하는 과정을 실습합니다. 이 함수는 신경망을 학습시키는 데 필수적인 구성 요소입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83df583",
      "metadata": {
        "id": "c83df583"
      },
      "source": [
        "## 04-005 모델 평가 함수 구현\n",
        "- 이 실습에서는 모델을 평가하기 위한 evaluate() 함수를 구현합니다. 모델을 평가 모드로 설정하고, 기울기 계산을 하지 않도록 torch.no_grad() 블록을 사용합니다. 테스트 데이터 로더에서 배치 단위로 데이터를 가져와 모델의 출력과 손실을 계산합니다. 마지막으로, 전체 테스트 손실을 계산하여 반환합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4309773",
      "metadata": {
        "id": "a4309773"
      },
      "outputs": [],
      "source": [
        "# 평가 함수 정의\n",
        "def evaluate(model, test_loader, criterion, device):\n",
        "    model.eval()  # 모델을 평가 모드로 설정\n",
        "\n",
        "    running_loss = 0.0 # 미니 배치별 loss값을 누적할 변수\n",
        "\n",
        "\n",
        "    with torch.no_grad():  # 평가 중에는 기울기 계산을 하지 않음\n",
        "        for datas, labels in test_loader: # 미니 배치 별 손실 계산\n",
        "            datas, labels = datas.to(device), labels.to(device) # 미니 배치별 데이터와 레이블 장치 할당\n",
        "\n",
        "            # 순전파\n",
        "            outputs = model(datas)\n",
        "\n",
        "            # 손실 계산\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # 손실 누적\n",
        "            running_loss += loss.item()\n",
        "\n",
        "\n",
        "    # 현재 Epoch의 평균 손실 값 계산 및 반환\n",
        "    return running_loss / len(test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92da3fb0",
      "metadata": {
        "id": "92da3fb0"
      },
      "source": [
        "- 모델 평가를 위한 evaluate() 함수를 구현하고, 테스트 데이터에서 손실을 반환하는 과정을 실습합니다. 이 함수는 모델의 성능을 측정하는 데 필수적입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dc0232e",
      "metadata": {
        "id": "4dc0232e"
      },
      "source": [
        "## 04-006 손실 함수 및 옵티마이저 설정과 학습 수행\n",
        "- 이 실습에서는 모델 학습을 위한 손실 함수와 옵티마이저를 설정합니다. nn.BCELoss()는 이진 분류 문제에 적합한 손실 함수로, 예측값이 1일 확률을 기반으로 손실을 계산합니다. Adam 옵티마이저는 학습률(lr)을 0.01로 설정하여 모델의 파라미터를 업데이트합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56231e3e",
      "metadata": {
        "id": "56231e3e"
      },
      "outputs": [],
      "source": [
        "# 손실 함수와 옵티마이저 설정\n",
        "criterion = nn.BCELoss()  # 이진 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam 옵티마이저"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cf72d22",
      "metadata": {
        "id": "7cf72d22"
      },
      "source": [
        "- 손실 함수와 옵티마이저를 설정하는 과정을 실습합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bb3cb2d",
      "metadata": {
        "id": "2bb3cb2d"
      },
      "source": [
        "## 04-007 학습 및 평가\n",
        "- 이 실습에서는 주어진 에포크 수만큼 학습을 반복하고, 매 에포크마다 학습 손실을 기록합니다. 학습 후에는 테스트 데이터셋에서 모델을 평가하고 손실을 기록합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60453f7e",
      "metadata": {
        "id": "60453f7e"
      },
      "outputs": [],
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# 학습 횟수 만큼 반복\n",
        "for epoch in range(100):\n",
        "\n",
        "    # 모델 학습(학습데이터)\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 모델 평가 (평가데이터)\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Train Loss : {train_loss} Test Loss : {test_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dabc99c9",
      "metadata": {
        "id": "dabc99c9"
      },
      "source": [
        "- 모델의 학습 및 평가 과정을 수행합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98743a9d",
      "metadata": {
        "id": "98743a9d"
      },
      "source": [
        "## 04-008 드롭아웃을 포함한 모델 클래스 구현\n",
        "- 이 실습에서는 드롭아웃(Dropout) 기법을 포함한 모델 클래스를 구현합니다. nn.Dropout()을 사용하여 과적합을 방지하기 위해 첫 번째 은닉층 뒤에 드롭아웃을 적용합니다. 드롭아웃 비율은 0.1로 설정하여 학습 중 무작위로 뉴런을 비활성화합니다. 모델의 구조는 summary() 함수를 통해 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f93839",
      "metadata": {
        "id": "50f93839"
      },
      "outputs": [],
      "source": [
        "# 신경망 모델 정의 (Dropout 포함)\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(7, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "        self.dropout = nn.Dropout(0.1)  # 10%의 드롭아웃 적용\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # 첫 번째 은닉층 뒤에 드롭아웃 적용\n",
        "        x = self.fc2(x)  # 출력층에는 드롭아웃을 사용하지 않음\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# 모델 인스턴스화\n",
        "model = DNN().to(device)\n",
        "summary(model, (32, 7))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a696489",
      "metadata": {
        "id": "4a696489"
      },
      "source": [
        "- 드롭아웃을 포함한 신경망 모델을 구현하고, 요약 정보를 통해 모델의 구조와 파라미터 수를 확인하는 과정을 실습합니다. 드롭아웃은 신경망의 일반화 성능을 향상시키는 데 효과적인 기법입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20ece524",
      "metadata": {
        "id": "20ece524"
      },
      "source": [
        "## 04-009 손실 함수 및 옵티마이저 설정과 학습 수행\n",
        "- 이 실습에서는 모델 학습을 위한 손실 함수와 옵티마이저를 설정합니다. nn.CrossEntropyLoss()는 다중 클래스 분류 문제에 적합한 손실 함수로, 각 클래스의 확률을 기반으로 손실을 계산합니다. Adam 옵티마이저는 학습률(lr)을 0.01로 설정하여 모델의 파라미터를 업데이트합니다. 이후, train() 함수와 evaluate() 함수를 호출하여 모델을 학습하고 평가합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d99d27db",
      "metadata": {
        "id": "d99d27db"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()  # 다중 클래스 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam 옵티마이저\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# 학습 횟수 만큼 반복\n",
        "for epoch in range(100):\n",
        "\n",
        "    # 모델 학습(학습데이터)\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 모델 평가 (평가데이터)\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Train Loss : {train_loss} Test Loss : {test_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0b6eb23",
      "metadata": {
        "id": "b0b6eb23"
      },
      "source": [
        "- 손실 함수와 옵티마이저를 설정한 후, 모델을 학습하고 평가하는 과정을 실습합니다. 이 과정은 신경망 모델의 성능을 최적화하는 데 필수적입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e2099f8",
      "metadata": {
        "id": "4e2099f8"
      },
      "source": [
        "## 04-010 Batch Normalization 및 Dropout 포함 모델 정의\n",
        "- 이 실습에서는 Batch Normalization과 Dropout을 포함한 모델을 정의합니다. nn.BatchNorm1d()를 사용하여 첫 번째 선형 레이어의 출력을 정규화하고, nn.Dropout()을 통해 과적합을 방지하기 위해 드롭아웃을 적용합니다. 모델의 구조는 summary() 함수를 통해 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5046a2ed",
      "metadata": {
        "id": "5046a2ed"
      },
      "outputs": [],
      "source": [
        "# 신경망 모델 정의 (Batch Normalization 및 Dropout 포함)\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(7, 32)\n",
        "        self.bn1 = nn.BatchNorm1d(32)  # 첫 번째 배치 정규화 레이어\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "        self.dropout = nn.Dropout(0.1)  # 10%의 드롭아웃 적용\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.fc1(x)))  # 첫 번째 배치 정규화 + ReLU\n",
        "        x = self.dropout(x)  # 드롭아웃 적용\n",
        "        x = self.fc2(x)  # 출력층에는 배치 정규화 및 드롭아웃을 사용하지 않음\n",
        "        x = self.sigmoid(x)\n",
        "        return x\n",
        "\n",
        "# 모델 인스턴스화\n",
        "model = DNN().to(device)\n",
        "summary(model, (32, 7))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dcc40f",
      "metadata": {
        "id": "d5dcc40f"
      },
      "source": [
        "- Batch Normalization 및 Dropout을 포함한 신경망 모델을 구현하고, 요약 정보를 통해 모델의 구조와 파라미터 수를 확인하는 과정을 실습합니다. 이 기법들은 신경망의 학습 성능을 향상시키는 데 도움을 줍니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2714025",
      "metadata": {
        "id": "f2714025"
      },
      "source": [
        "## 04-011 손실 함수 및 옵티마이저 설정과 학습 수행\n",
        "- 이 실습에서는 모델 학습을 위한 손실 함수와 옵티마이저를 설정합니다. nn.CrossEntropyLoss()는 다중 클래스 분류 문제에 적합한 손실 함수로, 각 클래스의 확률을 기반으로 손실을 계산합니다. Adam 옵티마이저는 학습률(lr)을 0.01로 설정하여 모델의 파라미터를 업데이트합니다. 이후, train() 함수와 evaluate() 함수를 호출하여 모델을 학습하고 평가합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c5076dc",
      "metadata": {
        "id": "6c5076dc"
      },
      "outputs": [],
      "source": [
        "criterion = nn.BCELoss()  # 다중 클래스 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam 옵티마이저\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# 학습 횟수 만큼 반복\n",
        "for epoch in range(100):\n",
        "\n",
        "    # 모델 학습(학습데이터)\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 모델 평가 (평가데이터)\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Train Loss : {train_loss} Test Loss : {test_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "def8cd43",
      "metadata": {
        "id": "def8cd43"
      },
      "source": [
        "- 손실 함수와 옵티마이저를 설정한 후, 모델을 학습하고 평가하는 과정을 실습합니다. 이 과정은 신경망 모델의 성능을 최적화하는 데 필수적입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45d12769",
      "metadata": {
        "id": "45d12769"
      },
      "source": [
        "## 04-012 L2 정칙화를 위한 옵티마이저 설정\n",
        "- 이 실습에서는 L2 정칙화(L2 regularization)를 적용하기 위해 SGD(Stochastic Gradient Descent) 옵티마이저를 설정합니다. weight_decay 매개변수를 사용하여 L2 정칙화 항을 추가함으로써 과적합을 방지할 수 있습니다. 이는 모델의 일반화 성능을 향상시키는 데 중요한 역할을 합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ad19821",
      "metadata": {
        "id": "5ad19821"
      },
      "outputs": [],
      "source": [
        "# L2 정칙화를 위한 옵티마이저 설정 (weight_decay가 L2 정칙화)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c216865",
      "metadata": {
        "id": "8c216865"
      },
      "source": [
        "- L2 정칙화를 적용한 옵티마이저를 설정하는 과정을 실습합니다. 이는 신경망 학습에서 모델의 일반화 성능을 개선하는 데 효과적입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deefe689",
      "metadata": {
        "id": "deefe689"
      },
      "source": [
        "## 연습문제-04-001 Sequential 방식으로 2차원 더미 데이터를 이용한 분류 모델 설계\n",
        "- 이 실습에서는 PyTorch의 Sequential API를 사용하여 분류 모델을 설계하는 방법을 학습합니다. Sequential 방식은 모델의 각 계층을 순차적으로 정의하며, 간단한 네트워크를 설계할 때 유용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef1ecd2d",
      "metadata": {
        "id": "ef1ecd2d"
      },
      "outputs": [],
      "source": [
        "# 첫 번째 모델은 nn.Sequential을 사용하여 정의됨.\n",
        "# nn.Sequential은 모델의 레이어를 순차적으로 쌓을 수 있는 간단한 방식.\n",
        "\n",
        "\n",
        "model2 = nn.Sequential(\n",
        "    nn.Linear(2, 32),  # 입력 2차원, 출력 32차원\n",
        "    nn.ReLU(),         # 활성화 함수\n",
        "    nn.Linear(32, 16), # 출력 16차원\n",
        "    nn.ReLU(),         # 활성화 함수\n",
        "    nn.Linear(16, 1),  # 출력 1차원\n",
        "    nn.Sigmoid()       # 시그모이드 함수로 확률 분포 반환\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "201ef596",
      "metadata": {
        "id": "201ef596"
      },
      "source": [
        "- PyTorch Sequential API를 사용하여 2차원 더미 데이터를 기반으로 다중 분류 DNN 모델을 설계하고, 각 계층의 역할과 모델 구조를 학습합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55fc51e3",
      "metadata": {
        "id": "55fc51e3"
      },
      "source": [
        "## 연습문제-04-002 Module 방식으로 2차원 더미 데이터를 이용한 회귀 모델 설계\n",
        "- 이 실습에서는 PyTorch의 Module 방식을 사용하여 회귀 문제를 해결하는 모델을 설계하는 방법을 학습합니다. Module 방식을 사용하면, 모델의 계층을 더 세밀하게 정의할 수 있으며, 복잡한 네트워크 구조나 맞춤형 로직을 적용할 때 유리합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "622da1fa",
      "metadata": {
        "id": "622da1fa"
      },
      "outputs": [],
      "source": [
        "# Module 방식으로 회귀 모델 설계\n",
        "class DNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = nn.Linear(2, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.layer1(x))\n",
        "        x = self.relu(self.layer2(x))\n",
        "        x = self.layer3(x)\n",
        "        return x\n",
        "model3 = DNN()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62784df0",
      "metadata": {
        "id": "62784df0"
      },
      "source": [
        "- PyTorch Module 클래스를 사용하여 2차원 더미 데이터를 기반으로 회귀 모델을 설계하고, 각 계층의 역할과 활성화 함수의 중요성을 학습합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdf20831",
      "metadata": {
        "id": "cdf20831"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a81c7d",
      "metadata": {
        "id": "02a81c7d"
      },
      "source": [
        "# 05. 합성곱신경망 (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b0683cd",
      "metadata": {
        "id": "2b0683cd"
      },
      "source": [
        "## 05-001 2D 컨볼루션 출력 크기 계산 함수 구현\n",
        "- 이 실습에서는 2D 컨볼루션 레이어의 출력 크기를 계산하는 함수를 구현합니다. 입력 크기, 커널 크기, 스트라이드, 패딩을 매개변수로 받아 컨볼루션 계산 공식을 사용하여 출력 높이와 너비를 반환합니다. 이 함수는 CNN 모델 설계 시 출력 크기를 예측하는 데 유용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3d16a4",
      "metadata": {
        "id": "0d3d16a4"
      },
      "outputs": [],
      "source": [
        "def conv2d_output_size(input_size, kernel_size, stride=1, padding=0):\n",
        "    height, width = input_size\n",
        "\n",
        "    # Convolution 공식 적용\n",
        "    out_height = (height + 2 * padding - kernel_size) // stride + 1\n",
        "    out_width = (width + 2 * padding - kernel_size) // stride + 1\n",
        "\n",
        "    return out_height, out_width"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d_output_size((128, 128), 3, 1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAT6A2ud9cBW",
        "outputId": "976c44f8-7c1e-41f4-8ea4-111f77fd4f1d"
      },
      "id": "qAT6A2ud9cBW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(126, 126)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv2d_output_size((63, 63), 3, 1, 0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfJ3fwjD9vMW",
        "outputId": "8a0cc56f-7375-4435-8982-f377cb357261"
      },
      "id": "SfJ3fwjD9vMW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(61, 61)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9742465",
      "metadata": {
        "id": "c9742465"
      },
      "source": [
        "- 2D 컨볼루션의 출력 크기를 계산하는 함수를 구현하는 과정을 실습합니다. 이 함수는 신경망 설계 시 중요한 역할을 합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121ecbc1",
      "metadata": {
        "id": "121ecbc1"
      },
      "source": [
        "## 05-002 컨볼루션 출력 크기 계산\n",
        "- 이 실습에서는 앞서 구현한 conv2d_output_size() 함수를 사용하여 컨볼루션 레이어의 출력 크기를 계산합니다. 입력으로 28x28 크기의 이미지와 3x3 커널, 스트라이드 1, 패딩 0을 설정하여 출력 feature map의 크기를 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beb78a03",
      "metadata": {
        "id": "beb78a03"
      },
      "outputs": [],
      "source": [
        "# 입력 크기 (Height, Width), 커널 크기, 스트라이드, 패딩\n",
        "output_size = conv2d_output_size((28, 28), 3, 1, 0)\n",
        "print(f\"Output feature map size: {output_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7db9a638",
      "metadata": {
        "id": "7db9a638"
      },
      "source": [
        "- 입력 이미지와 커널의 크기를 사용하여 컨볼루션의 출력 크기를 계산하고 확인하는 과정을 실습합니다. 이를 통해 CNN 모델의 구조를 이해하는 데 도움이 됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a79a7c65",
      "metadata": {
        "id": "a79a7c65"
      },
      "source": [
        "## 05-003 패딩을 포함한 컨볼루션 출력 크기 계산\n",
        "- 이 실습에서는 conv2d_output_size() 함수를 사용하여 패딩을 포함한 컨볼루션 레이어의 출력 크기를 계산합니다. 입력으로 28x28 크기의 이미지와 3x3 커널, 스트라이드 1, 패딩 1을 설정하여 출력 feature map의 크기를 확인합니다. 이 과정은 CNN 설계 시 출력 크기를 예측하는 데 중요합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "208b8545",
      "metadata": {
        "id": "208b8545"
      },
      "outputs": [],
      "source": [
        "# 입력 크기 (Height, Width), 커널 크기, 스트라이드, 패딩\n",
        "output_size = conv2d_output_size((28, 28), 3, 1, 1)\n",
        "print(f\"Output feature map size: {output_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ee530ec",
      "metadata": {
        "id": "8ee530ec"
      },
      "source": [
        "- 패딩을 포함하여 컨볼루션의 출력 크기를 계산하고 확인하는 과정을 실습합니다. 패딩의 효과를 이해하는 데 도움이 됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "458495ac",
      "metadata": {
        "id": "458495ac"
      },
      "source": [
        "## 05-004 스트라이드를 포함한 컨볼루션 출력 크기 계산\n",
        "- 이 실습에서는 conv2d_output_size() 함수를 사용하여 스트라이드를 포함한 컨볼루션 레이어의 출력 크기를 계산합니다. 입력으로 28x28 크기의 이미지와 3x3 커널, 스트라이드 2, 패딩 0을 설정하여 출력 feature map의 크기를 확인합니다. 이 과정은 CNN 모델 설계 시 중요한 요소입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd37ecf5",
      "metadata": {
        "id": "bd37ecf5"
      },
      "outputs": [],
      "source": [
        "# 입력 크기 (Height, Width), 커널 크기, 스트라이드, 패딩\n",
        "output_size = conv2d_output_size((28, 28), 3, 2, 0)\n",
        "print(f\"Output feature map size: {output_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09cf9131",
      "metadata": {
        "id": "09cf9131"
      },
      "source": [
        "- 스트라이드를 포함하여 컨볼루션의 출력 크기를 계산하고 확인하는 과정을 실습합니다. 스트라이드의 효과를 이해하는 데 도움이 됩니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c0e4b28",
      "metadata": {
        "id": "1c0e4b28"
      },
      "source": [
        "## 05-005 스트라이드 및 패딩을 포함한 컨볼루션 출력 크기 계산\n",
        "- 이 실습에서는 conv2d_output_size() 함수를 사용하여 스트라이드와 패딩을 모두 포함한 컨볼루션 레이어의 출력 크기를 계산합니다. 입력으로 28x28 크기의 이미지와 3x3 커널, 스트라이드 2, 패딩 1을 설정하여 출력 feature map의 크기를 확인합니다. 이 과정은 CNN 설계 시 중요한 요소입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94bb7331",
      "metadata": {
        "id": "94bb7331"
      },
      "outputs": [],
      "source": [
        "# 입력 크기 (Height, Width), 커널 크기, 스트라이드, 패딩\n",
        "output_size = conv2d_output_size((28, 28), 3, 2, 1)\n",
        "print(f\"Output feature map size: {output_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f326bda",
      "metadata": {
        "id": "8f326bda"
      },
      "source": [
        "- 스트라이드와 패딩을 포함하여 컨볼루션의 출력 크기를 계산하고 확인하는 과정을 실습합니다. 이를 통해 CNN의 동작을 더 깊이 이해할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73ea86f9",
      "metadata": {
        "id": "73ea86f9"
      },
      "source": [
        "## 05-006 PyTorch 및 관련 모듈 임포트와 device 설정\n",
        "- 이 실습에서는 PyTorch와 관련된 모듈을 임포트하고, 신경망을 GPU 또는 CPU에서 실행할 수 있도록 device를 설정하는 방법을 학습합니다. torch.device()를 사용하여 GPU 사용 가능 여부에 따라 적절한 장치를 선택합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2afdcbef",
      "metadata": {
        "id": "2afdcbef"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install JAEN -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "440d2d23",
      "metadata": {
        "id": "440d2d23"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchinfo import summary\n",
        "from JAEN.utils import plot_training_results\n",
        "\n",
        "# device 설정 (GPU가 사용 가능하면 GPU로, 그렇지 않으면 CPU 사용)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebb5d7e8",
      "metadata": {
        "id": "ebb5d7e8"
      },
      "source": [
        "- PyTorch의 다양한 모듈을 임포트하고, 학습에 사용할 장치(GPU 또는 CPU)를 설정하는 과정을 실습합니다. 이를 통해 GPU 가속을 활용한 빠른 연산이 가능해집니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60a22291",
      "metadata": {
        "id": "60a22291"
      },
      "source": [
        "## 05-007 FashionMNIST 데이터 변환 및 정규화\n",
        "- 이 실습에서는 FashionMNIST 데이터를 신경망에 입력할 수 있도록 텐서로 변환하고, 데이터를 정규화하는 방법을 학습합니다. transforms.ToTensor()는 이미지를 텐서로 변환하고, transforms.Normalize()는 평균과 표준편차를 이용해 데이터를 정규화하여 모델 학습 시 안정성을 높입니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fbe7945",
      "metadata": {
        "id": "8fbe7945"
      },
      "outputs": [],
      "source": [
        "# FashionMNIST 데이터 변환 (이미지를 텐서로 변환하고 [0, 1] 범위로 정규화)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34af551",
      "metadata": {
        "id": "a34af551"
      },
      "source": [
        "- FashionMNIST 데이터를 텐서로 변환하고, 정규화를 적용하는 과정을 실습합니다. 데이터 정규화는 모델의 학습 성능을 향상시키는 중요한 전처리 과정입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ba35534",
      "metadata": {
        "id": "5ba35534"
      },
      "source": [
        "## 05-008 FashionMNIST 학습 및 테스트 데이터셋 로드\n",
        "- 이 실습에서는 FashionMNIST 데이터셋을 학습용과 테스트용으로 로드하는 방법을 학습합니다. train=True로 설정된 데이터셋은 학습용 데이터로, train=False로 설정된 데이터셋은 테스트용 데이터로 로드됩니다. transform을 통해 앞서 정의한 이미지 변환 및 정규화가 적용됩니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4585d5c",
      "metadata": {
        "id": "f4585d5c"
      },
      "outputs": [],
      "source": [
        "# 학습 및 테스트 데이터셋 로드\n",
        "train_dataset = datasets.FashionMNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.FashionMNIST(root='./data', train=False, transform=transform, download=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7327716b",
      "metadata": {
        "id": "7327716b"
      },
      "source": [
        "- FashionMNIST 데이터셋을 학습용과 테스트용으로 로드하는 과정을 실습합니다. 이는 신경망 학습 및 평가를 위한 필수적인 단계입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db281116",
      "metadata": {
        "id": "db281116"
      },
      "source": [
        "## 05-009 데이터 로더 생성\n",
        "- 이 실습에서는 학습 및 테스트 데이터셋을 배치 단위로 로드하기 위해 DataLoader를 생성하는 방법을 학습합니다. train_loader는 배치 크기가 64이고 데이터를 무작위로 섞으며, test_loader는 배치 크기가 64이고 데이터를 순차적으로 로드합니다. DataLoader는 대규모 데이터셋을 처리할 때 매우 유용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3b53ff",
      "metadata": {
        "id": "8a3b53ff"
      },
      "outputs": [],
      "source": [
        "# 데이터 로더 생성\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff8f0170",
      "metadata": {
        "id": "ff8f0170"
      },
      "source": [
        "- 학습 및 테스트 데이터셋을 처리할 DataLoader를 생성하는 과정을 실습합니다. 배치 처리를 통해 메모리 효율성을 높이고, 데이터셋을 효과적으로 로드할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "945e3130",
      "metadata": {
        "id": "945e3130"
      },
      "source": [
        "## 05-010 CNN 모델 클래스 정의\n",
        "- 이 실습에서는 CNN(Convolutional Neural Network) 모델을 정의합니다. 두 개의 컨볼루션 레이어, 각 레이어 뒤에 배치 정규화 및 드롭아웃을 적용하고, 최종적으로 두 개의 완전 연결 레이어를 사용하여 FashionMNIST 데이터셋의 분류 문제를 해결합니다. summary() 함수를 통해 모델 구조를 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "117282ca",
      "metadata": {
        "id": "117282ca"
      },
      "outputs": [],
      "source": [
        "# Sequential로 모델 정의\n",
        "model = nn.Sequential(\n",
        "    # 첫 번째 Conv + ReLU + MaxPool\n",
        "    nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "    # 두 번째 Conv + ReLU + MaxPool\n",
        "    nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
        "    nn.ReLU(),\n",
        "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "\n",
        "    # Flatten\n",
        "    nn.Flatten(),\n",
        "\n",
        "    # 첫 번째 Fully Connected + ReLU\n",
        "    nn.Linear(32 * 7 * 7, 128),\n",
        "    nn.ReLU(),\n",
        "\n",
        "    # 두 번째 Fully Connected (출력층)\n",
        "    nn.Linear(128, 10)\n",
        ")\n",
        "model = model.to(device)\n",
        "summary(model, input_size=(64, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3621a56d",
      "metadata": {
        "id": "3621a56d"
      },
      "outputs": [],
      "source": [
        "# CNN 모델 정의\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 첫 번째 컨볼루션 레이어\n",
        "        # 입력 채널: 1 (흑백 이미지), 출력 채널: 16, 커널 크기: 3x3, 패딩: 1\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)\n",
        "\n",
        "        # 두 번째 컨볼루션 레이어\n",
        "        # 입력 채널: 16, 출력 채널: 32, 커널 크기: 3x3, 패딩: 1\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "\n",
        "        # MaxPool 레이어 (다운샘플링)\n",
        "        # 커널 크기: 2x2, 스트라이드: 2\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        # 첫 번째 완전 연결 (Fully Connected) 레이어\n",
        "        # 입력 크기: 32 * 7 * 7 (Conv2d 출력을 펼친 크기), 출력 크기: 128\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "\n",
        "        # 두 번째 완전 연결 레이어\n",
        "        # 입력 크기: 128, 출력 크기: 10 (클래스 개수)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "        # 활성화 함수\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 입력 데이터 크기: (batch_size, 1, 28, 28)\n",
        "\n",
        "        # 첫 번째 Conv + ReLU + MaxPool\n",
        "        # Conv 후 크기: (batch_size, 16, 28, 28)\n",
        "        # MaxPool 후 크기: (batch_size, 16, 14, 14)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "\n",
        "        # 두 번째 Conv + ReLU + MaxPool\n",
        "        # Conv 후 크기: (batch_size, 32, 14, 14)\n",
        "        # MaxPool 후 크기: (batch_size, 32, 7, 7)\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "\n",
        "        # Flatten: Conv 출력을 1차원 벡터로 펼침\n",
        "        # Flatten 후 크기: (batch_size, 32 * 7 * 7)\n",
        "        x = x.reshape(-1, 32 * 7 * 7)\n",
        "\n",
        "        # 첫 번째 Fully Connected + ReLU\n",
        "        # 출력 크기: (batch_size, 128)\n",
        "        x = self.relu(self.fc1(x))\n",
        "\n",
        "        # 두 번째 Fully Connected (출력층)\n",
        "        # 출력 크기: (batch_size, 10)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "# 모델 요약 출력\n",
        "model = CNN().to(device)\n",
        "summary(model, input_size=(64, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1480225",
      "metadata": {
        "id": "e1480225"
      },
      "source": [
        "- CNN 모델 클래스를 구현하고, 요약 정보를 통해 모델의 구조와 파라미터 수를 확인하는 과정을 실습합니다. CNN은 이미지 분류 문제에 효과적인 신경망 구조입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01c45b4c",
      "metadata": {
        "id": "01c45b4c"
      },
      "source": [
        "## 05-011 손실 함수 및 옵티마이저 설정과 학습 수행\n",
        "- 이 실습에서는 모델 학습을 위한 손실 함수와 옵티마이저를 설정합니다. nn.CrossEntropyLoss()는 다중 클래스 분류 문제에 적합한 손실 함수로, 각 클래스의 확률을 기반으로 손실을 계산합니다. Adam 옵티마이저는 학습률(lr)을 0.0001로 설정하여 모델의 파라미터를 업데이트합니다. 이후, train() 함수와 evaluate() 함수를 호출하여 모델을 학습하고 평가합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ba049a5",
      "metadata": {
        "id": "5ba049a5"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam 옵티마이저\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# 학습 횟수 만큼 반복\n",
        "for epoch in range(10):\n",
        "\n",
        "    # 모델 학습(학습데이터)\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 모델 평가 (평가데이터)\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Train Loss : {train_loss} Test Loss : {test_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4392cacd",
      "metadata": {
        "id": "4392cacd"
      },
      "source": [
        "- 손실 함수와 옵티마이저를 설정한 후, 모델을 학습하고 평가하는 과정을 실습합니다. 이 과정은 신경망 모델의 성능을 최적화하는 데 필수적입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44a193e",
      "metadata": {
        "id": "e44a193e"
      },
      "source": [
        "## 연습문제-05-001 (배치, 3, 32, 32) 입력을 받는 CNN 모델 설계 (Sequential 방식)\n",
        "- 이 실습에서는 PyTorch Sequential 방식을 사용하여 (배치, 3, 32, 32) 형태의 이미지 입력을 받는 CNN 모델을 설계하는 방법을 학습합니다. 이 CNN 모델은 세 개의 합성곱 층을 사용하며, 각 층의 뒤에는 ReLU 활성화 함수와 Max Pooling을 적용합니다. 모델은 64개의 필터를 가진 마지막 합성곱 층에서 특징을 추출하고, 이를 완전히 연결된 층으로 전달합니다. 마지막 출력은 10개의 클래스에 대한 확률값을 나타내며, 소프트맥스 활성화 함수를 사용하여 이를 계산합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "729d19f6",
      "metadata": {
        "id": "729d19f6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "model = nn.Sequential(\n",
        "            # 첫 번째 합성곱층과 풀링층\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),  # 입력 채널 3, 출력 채널 16, 커널 크기 3x3, 패딩 1\n",
        "            nn.ReLU(),  # 활성화 함수\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 풀링층: 출력 크기를 절반으로 축소 (출력 크기: (16, 16))\n",
        "\n",
        "            # 두 번째 합성곱층과 풀링층\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),  # 입력 채널 16, 출력 채널 32\n",
        "            nn.ReLU(),  # 활성화 함수\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 출력 크기를 절반으로 축소 (출력 크기: (8, 8))\n",
        "\n",
        "            # 세 번째 합성곱층과 풀링층\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 입력 채널 32, 출력 채널 64\n",
        "            nn.ReLU(),  # 활성화 함수\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),  # 출력 크기를 절반으로 축소 (출력 크기: (4, 4))\n",
        "\n",
        "            # Flatten 레이어\n",
        "            nn.Flatten(),  # (64, 4, 4) 형태의 출력을 이차원 텐서로 변환\n",
        "\n",
        "            # 첫 번째 완전 연결층\n",
        "            nn.Linear(64 * 4 * 4, 128),  # Conv 레이어 출력을 128차원 출력으로 변환\n",
        "            nn.ReLU(),  # 활성화 함수\n",
        "\n",
        "            # 최종 출력층\n",
        "            nn.Linear(128, 10)  # 10개의 클래스로 분류\n",
        "        )\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea595a8",
      "metadata": {
        "id": "7ea595a8"
      },
      "source": [
        "- (배치, 3, 32, 32) 형태의 입력을 받는 CNN 모델을 Sequential 방식으로 설계하여 이미지 분류 문제를 해결하는 방법을 실습합니다. CNN의 기본 구조와 작동 원리에 대해 익히며, 합성곱, ReLU 활성화 함수, Max Pooling, 그리고 완전 연결 층의 역할을 이해합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6295ad49",
      "metadata": {
        "id": "6295ad49"
      },
      "source": [
        "## 연습문제-05-002 (배치, 3, 224, 224) 입력을 받는 CNN 모델 설계 (Module 방식)\n",
        "- 이 실습에서는 PyTorch Module 방식을 사용하여 (배치, 3, 224, 224) 형태의 이미지 입력을 받는 CNN 모델을 설계하는 방법을 학습합니다. 이 모델은 더 큰 이미지(224x224 픽셀)를 처리할 수 있도록 설계되었으며, 네 개의 합성곱 층을 사용하여 이미지에서 점차적으로 더 추상적인 특징을 추출합니다. 각 합성곱 층 뒤에는 ReLU 활성화 함수와 Max Pooling을 적용하여 공간 정보를 줄이면서도 유용한 특징을 남깁니다. 마지막 완전 연결 층에서 추출된 특징을 기반으로 최종적으로 2개의 클래스를 분류합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "694a69f7",
      "metadata": {
        "id": "694a69f7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # 첫 번째 합성곱층과 풀링층\n",
        "        self.conv1 = nn.Conv2d(3, 256, kernel_size=3, padding=1)  # 입력 채널 3, 출력 채널 256, 커널 크기 3x3, 패딩 1\n",
        "        self.relu1 = nn.ReLU()  # 활성화 함수\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)  # 풀링층: 출력 크기를 (112, 112)로 축소\n",
        "\n",
        "        # 두 번째 합성곱층과 풀링층\n",
        "        self.conv2 = nn.Conv2d(256, 128, kernel_size=3, padding=1)  # 입력 채널 256, 출력 채널 128\n",
        "        self.relu2 = nn.ReLU()  # 활성화 함수\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)  # 출력 크기를 (56, 56)으로 축소\n",
        "\n",
        "        # 세 번째 합성곱층과 풀링층\n",
        "        self.conv3 = nn.Conv2d(128, 64, kernel_size=3, padding=1)  # 입력 채널 128, 출력 채널 64\n",
        "        self.relu3 = nn.ReLU()  # 활성화 함수\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)  # 출력 크기를 (28, 28)으로 축소\n",
        "\n",
        "        # 네 번째 합성곱층과 풀링층\n",
        "        self.conv4 = nn.Conv2d(64, 32, kernel_size=3, padding=1)  # 입력 채널 64, 출력 채널 32\n",
        "        self.relu4 = nn.ReLU()  # 활성화 함수\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)  # 출력 크기를 (14, 14)로 축소\n",
        "\n",
        "        # Flatten 레이어\n",
        "        self.flatten = nn.Flatten()  # (32, 14, 14) 형태의 출력을 일차원 벡터로 변환\n",
        "\n",
        "        # 첫 번째 완전 연결층\n",
        "        self.fc1 = nn.Linear(32 * 14 * 14, 256)  # Conv 레이어 출력을 낮은 차원으로 줄임\n",
        "        self.relu_fc1 = nn.ReLU()  # 활성화 함수\n",
        "\n",
        "        # 두 번째 완전 연결층\n",
        "        self.fc2 = nn.Linear(256, 2)  # 두 개의 클래스로 최종 분류\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 첫 번째 합성곱층과 풀링층\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # 두 번째 합성곱층과 풀링층\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # 세 번째 합성곱층과 풀링층\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # 네 번째 합성곱층과 풀링층\n",
        "        x = self.conv4(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        # Flatten 레이어\n",
        "        x = self.flatten(x)\n",
        "\n",
        "        # 첫 번째 완전 연결층\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu_fc1(x)\n",
        "\n",
        "        # 두 번째 완전 연결층 (최종 출력)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "model = CNN()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e30dc04c",
      "metadata": {
        "id": "e30dc04c"
      },
      "source": [
        "- (배치, 3, 224, 224) 형태의 입력을 받는 CNN 모델을 Sequential 방식으로 설계하여 고해상도 이미지 분류 문제를 해결하는 방법을 실습합니다. 더 깊은 네트워크 구조를 통해 고해상도 이미지를 처리하며, 각 층이 추출하는 특징과 네트워크의 학습 방식에 대해 이해합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f0e9c3",
      "metadata": {
        "id": "15f0e9c3"
      },
      "source": [
        "## 05-012 JAEN 패키지에서 CNN 모델 불러오기\n",
        "- 이 실습에서는 JAEN 패키지에서 제공하는 CNN 모델을 불러오고, 사전 학습된(pretrained) 가중치를 사용하여 모델을 초기화하는 방법을 학습합니다. summary() 함수를 사용하여 모델의 구조와 파라미터 수를 확인하여 모델이 올바르게 로드되었는지 검증합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af84d090",
      "metadata": {
        "id": "af84d090"
      },
      "outputs": [],
      "source": [
        "# JAEN 패키지에서 CNN 모델 가져오기\n",
        "from JAEN.models import CNNModel\n",
        "\n",
        "# CNN 모델 불러오기 (pretrained=True)\n",
        "model = CNNModel(pretrained=True)\n",
        "\n",
        "# 모델 정보 확인\n",
        "summary(model, (64, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "653ebfd4",
      "metadata": {
        "id": "653ebfd4"
      },
      "source": [
        "- JAEN 패키지에서 CNN 모델을 불러오고, 모델 정보를 요약하여 확인하는 과정을 실습합니다. 이는 사전 학습된 모델을 사용할 때 유용합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bca43b3",
      "metadata": {
        "id": "5bca43b3"
      },
      "source": [
        "## 05-013 기존 Conv Block 동결\n",
        "- 이 실습에서는 사전 학습된 CNN 모델의 일부 레이어(Conv Block)의 파라미터를 동결하여 학습 중에 업데이트되지 않도록 설정하는 방법을 학습합니다. 이는 transfer learning 기법의 일환으로, 특정 레이어의 가중치를 고정하여 새로운 데이터셋에 대해 학습할 때 유용합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "858027b5",
      "metadata": {
        "id": "858027b5"
      },
      "outputs": [],
      "source": [
        "# 기존 Conv Block 동결\n",
        "for param in model.conv_layers.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5a946e",
      "metadata": {
        "id": "4b5a946e"
      },
      "source": [
        "- CNN 모델의 기존 Conv Block을 동결하여 해당 레이어의 파라미터가 학습되지 않도록 설정하는 과정을 실습합니다. 이는 전이 학습 시 일반적인 기법입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a780005",
      "metadata": {
        "id": "8a780005"
      },
      "source": [
        "## 05-014 새로운 Fully Connected Block 설정\n",
        "- 이 실습에서는 CNN 모델의 Fully Connected Block을 새롭게 설정합니다. nn.Sequential()을 사용하여 새로운 은닉층과 드롭아웃 레이어를 추가하고, 출력층을 정의합니다. 모델을 디바이스(GPU 또는 CPU)로 이동한 후, summary() 함수를 사용하여 모델의 구조를 확인합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bb39dd9",
      "metadata": {
        "id": "3bb39dd9"
      },
      "outputs": [],
      "source": [
        "# 새로운 Fully Connected Block 설정\n",
        "model.fc_layers = nn.Sequential(\n",
        "    nn.Linear(32 * 7 * 7, 64),  # 첫 번째 은닉층\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(p=0.5),           # 드롭아웃 추가\n",
        "    nn.Linear(64, 10)           # 출력층 (활성화 함수 없음)\n",
        ")\n",
        "\n",
        "# 디바이스 설정 (GPU 또는 CPU)\n",
        "model = model.to(device)\n",
        "summary(model, input_size=(64, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77a657e5",
      "metadata": {
        "id": "77a657e5"
      },
      "source": [
        "- 새로운 Fully Connected Block을 설정하고, 모델의 구조와 파라미터 수를 요약하여 확인하는 과정을 실습합니다. 이는 모델 구조를 조정하고 최적화하는 데 중요한 단계입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e121bd0f",
      "metadata": {
        "id": "e121bd0f"
      },
      "source": [
        "## 05-015 손실 함수 및 최적화 도구 정의와 학습 수행\n",
        "- 이 실습에서는 모델 학습을 위한 손실 함수와 옵티마이저를 설정합니다. nn.CrossEntropyLoss()는 다중 클래스 분류 문제에 적합한 손실 함수로, 각 클래스의 확률을 기반으로 손실을 계산합니다. Adam 옵티마이저는 학습률(lr)을 0.0001로 설정하여 모델의 파라미터를 업데이트합니다. 이후, train() 함수와 evaluate() 함수를 호출하여 모델을 학습하고 평가합니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58caf05a",
      "metadata": {
        "id": "58caf05a"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam 옵티마이저\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# 학습 횟수 만큼 반복\n",
        "for epoch in range(10):\n",
        "\n",
        "    # 모델 학습(학습데이터)\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 모델 평가 (평가데이터)\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Train Loss : {train_loss} Test Loss : {test_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2074a03b",
      "metadata": {
        "id": "2074a03b"
      },
      "source": [
        "- 손실 함수와 최적화 도구를 설정한 후, 모델을 학습하고 평가하는 과정을 실습합니다. 이 과정은 신경망 모델의 성능을 최적화하는 데 필수적입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08dbd6a4",
      "metadata": {
        "id": "08dbd6a4"
      },
      "source": [
        "## 05-016 마지막 두 Conv 레이어만 학습하도록 설정\n",
        "- 이 실습에서는 CNN 모델에서 마지막 두 컨볼루션 레이어의 파라미터만 학습하도록 설정합니다. named_parameters() 메서드를 사용하여 각 레이어의 이름과 파라미터를 반복하고, 특정 레이어에 대해서만 requires_grad를 True로 설정합니다. 이를 통해 모델의 특정 부분만 학습하여 과적합을 방지하고 성능을 최적화할 수 있습니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0290a98b",
      "metadata": {
        "id": "0290a98b"
      },
      "outputs": [],
      "source": [
        "# 마지막 두 Conv 레이어만 학습하도록 설정\n",
        "for name, p in model.conv_layers.named_parameters():\n",
        "    if name in ['5.weight', '5.bias', '7.weight', '7.bias']:\n",
        "        p.requires_grad = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2902d928",
      "metadata": {
        "id": "2902d928"
      },
      "source": [
        "- CNN 모델의 마지막 두 Conv 레이어만 학습하도록 설정하는 과정을 실습합니다. 이는 전이 학습 시 효과적인 접근 방식입니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45ef1f49",
      "metadata": {
        "id": "45ef1f49"
      },
      "source": [
        "## 05-017 손실 함수 및 최적화 도구 정의와 학습 수행\n",
        "- 이 실습에서는 모델 학습을 위한 손실 함수와 옵티마이저를 설정합니다. nn.CrossEntropyLoss()는 다중 클래스 분류 문제에 적합한 손실 함수로, 각 클래스의 확률을 기반으로 손실을 계산합니다. Adam 옵티마이저는 학습률(lr)을 0.0001로 설정하여 모델의 파라미터를 업데이트합니다. 이후, train() 함수와 evaluate() 함수를 호출하여 모델을 학습하고 평가합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1262d6de",
      "metadata": {
        "id": "1262d6de"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()  # 다중 클래스 분류를 위한 손실 함수\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)  # Adam 옵티마이저\n",
        "\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "# 학습 횟수 만큼 반복\n",
        "for epoch in range(10):\n",
        "\n",
        "    # 모델 학습(학습데이터)\n",
        "    train_loss = train(model, train_loader, criterion, optimizer, device)\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    # 모델 평가 (평가데이터)\n",
        "    test_loss = evaluate(model, test_loader, criterion, device)\n",
        "    test_losses.append(test_loss)\n",
        "\n",
        "    print(f'Epoch {epoch+1} Train Loss : {train_loss} Test Loss : {test_loss}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd997fb5",
      "metadata": {
        "id": "cd997fb5"
      },
      "source": [
        "- 손실 함수와 최적화 도구를 설정한 후, 모델을 학습하고 평가하는 과정을 실습합니다. 이 과정은 신경망 모델의 성능을 최적화하는 데 필수적입니다.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}